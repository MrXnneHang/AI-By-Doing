{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "feb17090-5ed8-42c1-b5bb-8f7d081bc394",
   "metadata": {},
   "source": [
    "## Q-Learning  \n",
    "### 计算步骤：\n",
    "\n",
    "1. Q-Table的创建： 想象一个表格，行是小狮子的位置，列是它可以采取的动作（比如上、下、左、右）。每个单元格的值叫Q值，代表在这个位置采取这个动作的“好坏”。\n",
    "2. 更新Q值：\n",
    "\n",
    "* 小狮子选择一个动作，移动到下一个位置。\n",
    "* 得到奖励（如果找到火腿，加分；碰到陷阱，减分）。\n",
    "* 更新公式是：\n",
    "  新的Q值=旧的Q值+学习率×(奖励+未来最大Q值−旧的Q值)\n",
    "* 学习率决定了这次更新对旧值的影响有多大。\n",
    "## Linear Q-Learning\n",
    "### 计算步骤：\n",
    "\n",
    "1. 特征向量： 这里不再用表格，而是用一个简单的数学公式来表示Q值。想象小狮子的状态（位置）用几个数字来描述。\n",
    "2. 更新权重：\n",
    "\n",
    "* 使用特征向量来计算Q值。\n",
    "* 更新权重的方式类似：\n",
    "  新的权重=旧的权重+学习率×(奖励+未来最大Q值−当前Q值)×特征向量\n",
    "\n",
    "* 这里的“特征向量”就像是描述小狮子当前状态的数字组合。\n",
    "## 区别总结\n",
    "1. Q-Learning： 使用表格逐个记录每个状态和动作的Q值，每次更新一个单元格。\n",
    "2. Linear Q-Learning： 不用表格，而是用公式和数字组合来快速估算Q值，每次调整公式的参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715ff006-6d1f-49a1-9d96-2d7ebce7d3f2",
   "metadata": {},
   "source": [
    "## 详解特征向量和权重向量：\n",
    "\n",
    "ϕ(s)=[x,y,d]<br>\n",
    "\n",
    "x,y坐标，d是距离火腿的距离，实际上也可以d,d，两个距离都放进来。<br>\n",
    "\n",
    "### 权重向量和特征向量:<br>\n",
    "\n",
    "1. 特征向量： 代表当前状态的特征，通常是一个数组，如:<br>\n",
    "\n",
    "ϕ(s)=[a,b,c,d...]。<br>\n",
    "\n",
    "2. 权重向量： 存储与特征相关的权重，形状与特征向量相同，如 \n",
    "𝜃=[a1,b1,c1,d1...]。<br>\n",
    "\n",
    "权重向量就是Q-Table的Q-value,特征向量就是Q-Table,不过，特征向量和权重向量做了更精细的维度分解，x,y都有q-value,还有d,而不是one-hot了。<br>\n",
    "\n",
    "特征更复杂，维度更高的情况下，Q-Table会报废，而Linear-Qlearning会更能站一些。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a7bdc51-3d8a-4092-b5dc-5590b8f0554b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m display\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# 设置地图和相关参数\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "# 设置地图和相关参数\n",
    "grid_size = 5\n",
    "num_episodes = 10\n",
    "max_steps = 20\n",
    "learning_rate = 0.1\n",
    "discount_factor = 0.9\n",
    "exploration_rate = 1.0\n",
    "exploration_decay = 0.99\n",
    "\n",
    "# 初始化Q-Table\n",
    "q_table = np.zeros((grid_size, grid_size, 4))  # 4个动作：上、下、左、右\n",
    "\n",
    "# 定义动作\n",
    "UP, DOWN, LEFT, RIGHT = 0, 1, 2, 3\n",
    "\n",
    "# 定义环境状态\n",
    "fire_position = (3, 2)\n",
    "trap_position = (2, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708b3f04-7f28-451d-b95b-140c8c8a5d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_position(state, action):\n",
    "    x, y = state\n",
    "    if action == UP:\n",
    "        return (max(0, x - 1), y)\n",
    "    elif action == DOWN:\n",
    "        return (min(grid_size - 1, x + 1), y)\n",
    "    elif action == LEFT:\n",
    "        return (x, max(0, y - 1))\n",
    "    elif action == RIGHT:\n",
    "        return (x, min(grid_size - 1, y + 1))\n",
    "\n",
    "def get_reward(state):\n",
    "    if state == fire_position:\n",
    "        return 10  # 找到火腿\n",
    "    elif state == trap_position:\n",
    "        return -10  # 碰到陷阱\n",
    "    else:\n",
    "        return -1  # 每一步的惩罚\n",
    "\n",
    "def show_state(end, state, episode, step, q_table):\n",
    "    terminal = fire_position\n",
    "    hole = trap_position\n",
    "    env = np.array([[\"_ \"] * grid_size for _ in range(grid_size)])\n",
    "    env[terminal] = \"$ \"\n",
    "    env[hole] = \"# \"\n",
    "    env[state] = \"L \"\n",
    "    interaction = \"\"\n",
    "    for row in env:\n",
    "        interaction += \"\".join(row) + \"\\n\"\n",
    "\n",
    "    if state == terminal:\n",
    "        message = \"EPISODE: {}, STEP: {}\".format(episode, step)\n",
    "        interaction += message\n",
    "        display.clear_output(wait=True)\n",
    "        print(interaction)\n",
    "        print(\"\\n\" + \"q_table:\")\n",
    "        print(q_table)\n",
    "        time.sleep(3)\n",
    "    else:\n",
    "        display.clear_output(wait=True)\n",
    "        print(interaction)\n",
    "        print(q_table)\n",
    "        time.sleep(0.3)\n",
    "\n",
    "# 训练过程\n",
    "for episode in range(num_episodes):\n",
    "    state = (0, 0)  # 初始化位置\n",
    "    for step in range(max_steps):\n",
    "        # 选择动作\n",
    "        if random.uniform(0, 1) < exploration_rate:\n",
    "            action = random.randint(0, 3)  # 随机选择动作\n",
    "        else:\n",
    "            action = np.argmax(q_table[state[0], state[1]])  # 选择Q值最高的动作\n",
    "\n",
    "        next_state = get_next_position(state, action)\n",
    "        reward = get_reward(next_state)\n",
    "\n",
    "        # Q-Learning更新\n",
    "        q_table[state[0], state[1], action] += learning_rate * (\n",
    "            reward + discount_factor * np.max(q_table[next_state[0], next_state[1]]) - q_table[state[0], state[1], action]\n",
    "        )\n",
    "\n",
    "        # 更新状态\n",
    "        state = next_state\n",
    "\n",
    "        # 显示状态\n",
    "        show_state(state == fire_position, state, episode, step, q_table)\n",
    "\n",
    "    # 降低探索率\n",
    "    exploration_rate *= exploration_decay\n",
    "\n",
    "# 训练完成\n",
    "print(\"Training finished!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
