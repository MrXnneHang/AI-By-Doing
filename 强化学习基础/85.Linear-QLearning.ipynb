{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "feb17090-5ed8-42c1-b5bb-8f7d081bc394",
   "metadata": {},
   "source": [
    "## Q-Learning  \n",
    "### 计算步骤：\n",
    "\n",
    "1. Q-Table的创建： 想象一个表格，行是小狮子的位置，列是它可以采取的动作（比如上、下、左、右）。每个单元格的值叫Q值，代表在这个位置采取这个动作的“好坏”。\n",
    "2. 更新Q值：\n",
    "\n",
    "* 小狮子选择一个动作，移动到下一个位置。\n",
    "* 得到奖励（如果找到火腿，加分；碰到陷阱，减分）。\n",
    "* 更新公式是：\n",
    "  新的Q值=旧的Q值+学习率×(奖励+未来最大Q值−旧的Q值)\n",
    "* 学习率决定了这次更新对旧值的影响有多大。\n",
    "## Linear Q-Learning\n",
    "### 计算步骤：\n",
    "\n",
    "1. 特征向量： 这里不再用表格，而是用一个简单的数学公式来表示Q值。想象小狮子的状态（位置）用几个数字来描述。\n",
    "2. 更新权重：\n",
    "\n",
    "* 使用特征向量来计算Q值。\n",
    "* 更新权重的方式类似：\n",
    "  新的权重=旧的权重+学习率×(奖励+未来最大Q值−当前Q值)×特征向量\n",
    "\n",
    "* 这里的“特征向量”就像是描述小狮子当前状态的数字组合。\n",
    "## 区别总结\n",
    "1. Q-Learning： 使用表格逐个记录每个状态和动作的Q值，每次更新一个单元格。\n",
    "2. Linear Q-Learning： 不用表格，而是用公式和数字组合来快速估算Q值，每次调整公式的参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715ff006-6d1f-49a1-9d96-2d7ebce7d3f2",
   "metadata": {},
   "source": [
    "## 详解特征向量和权重向量：\n",
    "\n",
    "ϕ(s)=[x,y]<br>\n",
    "\n",
    "x,y坐标，是距离火腿的距离，实际上也再次加入d，代表距离火腿的距离，但不好写，独立做奖励机制。<br>\n",
    "\n",
    "### 权重向量和特征向量:<br>\n",
    "\n",
    "1. 特征向量： 代表当前状态的特征，通常是一个数组，如:<br>\n",
    "\n",
    "ϕ(s)=[a,b,c,d...]。<br>\n",
    "\n",
    "2. 权重向量： 存储与特征相关的权重，形状与特征向量相同，如 \n",
    "𝜃=[a1,b1,c1,d1...]。<br>\n",
    "\n",
    "权重向量就是Q-Table的Q-value,特征向量就是Q-Table,不过，特征向量和权重向量做了更精细的维度分解，x,y都有q-value,还有d,而不是one-hot了。<br>\n",
    "\n",
    "特征更复杂，维度更高的情况下，Q-Table会报废，而Linear-Qlearning会更能站一些。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a7bdc51-3d8a-4092-b5dc-5590b8f0554b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import pickle\n",
    "\n",
    "# 设置地图和相关参数\n",
    "grid_size = 5\n",
    "num_episodes = 10\n",
    "max_steps = 20\n",
    "learning_rate = 0.1\n",
    "discount_factor = 0.9\n",
    "exploration_rate = 1.0\n",
    "exploration_decay = 0.99\n",
    "\n",
    "# 初始化权重\n",
    "weights = np.zeros((grid_size, grid_size, 4))  # 权重数组\n",
    "\n",
    "# 定义动作\n",
    "UP, DOWN, LEFT, RIGHT = 0, 1, 2, 3\n",
    "\n",
    "# 定义环境状态\n",
    "fire_position = (3, 2)\n",
    "trap_position = (2, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "708b3f04-7f28-451d-b95b-140c8c8a5d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_position(state, action):\n",
    "    x, y = state\n",
    "    if action == UP:\n",
    "        return (max(0, x - 1), y)\n",
    "    elif action == DOWN:\n",
    "        return (min(grid_size - 1, x + 1), y)\n",
    "    elif action == LEFT:\n",
    "        return (x, max(0, y - 1))\n",
    "    elif action == RIGHT:\n",
    "        return (x, min(grid_size - 1, y + 1))\n",
    "\n",
    "def get_reward(state):\n",
    "    if state == fire_position:\n",
    "        return 10  # 找到火腿\n",
    "    elif state == trap_position:\n",
    "        return -10  # 碰到陷阱\n",
    "    else:\n",
    "        return -1  # 每一步的惩罚\n",
    "\n",
    "def get_q_value(state, action):\n",
    "    return weights[state[0], state[1], action]\n",
    "\n",
    "def update_weights(state, action, reward, next_state):\n",
    "    max_future_q = np.max(weights[next_state[0], next_state[1]])  # 未来最大Q值\n",
    "    td_target = reward + discount_factor * max_future_q  # 时间差分目标\n",
    "    td_error = td_target - get_q_value(state, action)  # TD误差\n",
    "    weights[state[0], state[1], action] += learning_rate * td_error  # 更新权重\n",
    "\n",
    "def show_state(end, state, episode, step, weights):\n",
    "    terminal = fire_position\n",
    "    hole = trap_position\n",
    "    env = np.array([[\"_ \"] * grid_size for _ in range(grid_size)])\n",
    "    env[terminal] = \"$ \"\n",
    "    env[hole] = \"# \"\n",
    "    env[state] = \"L \"\n",
    "    interaction = \"\"\n",
    "    for row in env:\n",
    "        interaction += \"\".join(row) + \"\\n\"\n",
    "\n",
    "\n",
    "    if state == terminal:\n",
    "        message = \"EPISODE: {}, STEP: {}\".format(episode, step)\n",
    "        interaction += message\n",
    "        display.clear_output(wait=True)\n",
    "        print(interaction)\n",
    "        print(\"\\n\" + \"weights:\")\n",
    "        print(weights)\n",
    "        time.sleep(3)\n",
    "    else:\n",
    "        display.clear_output(wait=True)\n",
    "        print(interaction)\n",
    "        print(weights)\n",
    "        time.sleep(0.3)\n",
    "\n",
    "def save_weights(filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(weights, f)\n",
    "    print(\"Weights saved.\")\n",
    "\n",
    "def load_weights(filename):\n",
    "    global weights\n",
    "    with open(filename, 'rb') as f:\n",
    "        weights = pickle.load(f)\n",
    "    print(\"Weights loaded.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bccb04-bf5b-4715-ba1f-85cc4ce749ef",
   "metadata": {},
   "source": [
    "## 解释:为什么权重向量的数量是width x height\n",
    "\n",
    "* 特征向量和权重向量一一对应，特征向量有几个，权重向量就有几个.\n",
    "* [x,y,d],都是由x,y决定的，有多少个位置，就有多少个特征向量\n",
    "\n",
    "Linear QLearning在应对更多维度的时候，把一种one hot转换成一种稠密的矩阵，这是它巨大的优势。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4d00aa99-6d27-4787-b05c-cf3cff7e7a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_ _ _ _ _ \n",
      "_ _ _ _ _ \n",
      "_ # _ _ _ \n",
      "_ L $ _ _ \n",
      "_ _ _ _ _ \n",
      "\n",
      "[[[-1.79230735 -1.48542175 -1.72827068 -1.49269051]\n",
      "  [-1.15888506 -0.70394507 -1.28465673 -0.96063847]\n",
      "  [-0.66132702 -0.30345737 -0.74104258 -0.566659  ]\n",
      "  [-0.199      -0.2760635  -0.31727692 -0.450829  ]\n",
      "  [-0.2881     -0.3439     -0.2881     -0.199     ]]\n",
      "\n",
      " [[-1.06089    -0.97137224 -1.0982497  -0.76442356]\n",
      "  [-0.99507371 -6.26563899 -0.91247028  0.74391092]\n",
      "  [-0.6959423   3.26465832 -0.5823732  -0.60706994]\n",
      "  [-0.361      -0.30742185  0.11063434 -0.19      ]\n",
      "  [-0.374851   -0.1        -0.20626388 -0.1       ]]\n",
      "\n",
      " [[-0.97539604 -0.61655751 -0.6480372  -2.7271    ]\n",
      "  [-0.53627971  1.06547553 -0.5538819   1.65636756]\n",
      "  [ 0.15094166  9.07682311 -6.39117318 -0.33470942]\n",
      "  [-0.19        1.1193342  -0.1        -0.19      ]\n",
      "  [-0.19       -0.1         0.         -0.1       ]]\n",
      "\n",
      " [[-0.27943803 -0.5307031  -0.347149    0.42300309]\n",
      "  [-1.         -0.07615865 -0.19171     6.7354628 ]\n",
      "  [ 4.01585836  0.84922268  1.3064509   1.62665789]\n",
      "  [-0.18516285 -0.18361729  7.06744549 -0.3529    ]\n",
      "  [-0.19       -0.1        -0.1        -0.271     ]]\n",
      "\n",
      " [[-0.274249   -0.199      -0.28       -0.33984813]\n",
      "  [ 0.         -0.22279534 -0.199       0.53560729]\n",
      "  [ 6.02720446  0.05440154 -0.17826192 -0.33751729]\n",
      "  [ 0.27689457 -0.271      -0.0271      0.        ]\n",
      "  [-0.109       0.          0.          0.        ]]]\n",
      "Weights saved.\n",
      "Training finished!\n"
     ]
    }
   ],
   "source": [
    "load_weights(\"./weights.pkl\")\n",
    "# 训练过程\n",
    "for episode in range(num_episodes):\n",
    "    state = (0, 0)  # 初始化位置\n",
    "    for step in range(max_steps):\n",
    "        if random.uniform(0, 1) < exploration_rate:\n",
    "            action = random.randint(0, 3)  # 随机选择动作\n",
    "        else:\n",
    "            action = np.argmax(weights[state[0], state[1]])  # 选择权重最高的动作\n",
    "\n",
    "        next_state = get_next_position(state, action)\n",
    "        reward = get_reward(next_state)\n",
    "\n",
    "        # 更新权重\n",
    "        update_weights(state, action, reward, next_state)\n",
    "\n",
    "        # 更新状态\n",
    "        state = next_state\n",
    "\n",
    "        # 显示状态\n",
    "        show_state(state == fire_position, state, episode, step, weights)\n",
    "\n",
    "    # 降低探索率\n",
    "    exploration_rate *= exploration_decay\n",
    "\n",
    "# 保存权重\n",
    "save_weights(\"weights.pkl\")\n",
    "\n",
    "# 训练完成\n",
    "print(\"Training finished!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262f5984-3351-42ab-b3c7-0257c2a47bfe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
