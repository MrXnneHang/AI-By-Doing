{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d40c2a6-607d-4436-a122-106eab62662c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 强化学习与监督学习的区别"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b72c6f7-d8de-481f-8bc2-fc13b85e4480",
   "metadata": {},
   "source": [
    "* 如果按照监督学习的方式，类似于大人会给小孩说，这样做是对的，那样做是错误的。然后，小孩就能知道怎样做是对的，怎样是错误的了。这个过程并不存在奖励机制，无法从环境中获得回报。\n",
    "* 如果按照强化学习的方式，类似于大人什么都不告诉小孩，当小孩说脏话时就会挨打（负奖励），小孩做正确的事情时就给一颗糖吃（正奖励）。最后，小孩也就建立起是非观念了。为了得到更多的糖吃，小孩就会尽量避免做错误的事情。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925afdc0-0577-43d2-8ee2-98c08b575815",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 分类:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2610687-cad4-4bb8-89f6-4f5398441e7c",
   "metadata": {},
   "source": [
    "Q-learning\n",
    "\n",
    "SARSA\n",
    "\n",
    "DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3995d6be-bc1f-4b48-a412-f9478194d0c8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 组件:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa4936a-66bf-436a-b73b-ba9800e71b95",
   "metadata": {},
   "source": [
    "### Policy 策略函数:\n",
    "\n",
    "当前状态作为输入，下一步行动决策作为输出。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fec49d3-854a-418c-b999-43b4cc6aa04a",
   "metadata": {},
   "source": [
    "### Value Function:\n",
    "\n",
    "衡量当前状态或者行为的好坏，奖励和惩罚的函数。通常是一个value。<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6eeb880-92d1-450d-81c3-8963a812e16c",
   "metadata": {},
   "source": [
    "### Model:\n",
    "\n",
    "智能体(Agent)感知周围环境变化的模式。<br>\n",
    "\n",
    "Agent并不是必须，比如Q-Learning就是不需要智能体的。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b965537a-c012-475d-8b4f-131efe6bd773",
   "metadata": {},
   "source": [
    "## 此次强化学习的全部课程集中关注于时间差分学习的部分，并着重于 Q-learning、Sarsa 以及 Policy Gradient 三种最基础的方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e493517b-7f2b-4b29-9694-3b489debdd83",
   "metadata": {},
   "source": [
    "## !!只是基础!!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5314bf1-28c7-4b14-bc63-258e3535fdfd",
   "metadata": {},
   "source": [
    "# Q-Learning:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befa81b2-37bd-4a53-bb59-813e9a10ec7e",
   "metadata": {},
   "source": [
    "![](https://cdn.aibydoing.com/aibydoing/images/document-uid214893labid6102timestamp1531891479974.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc83a820-296b-4555-97c6-eb85307be380",
   "metadata": {},
   "source": [
    "* 我们不希望狮子碰到地雷，希望他吃到火腿。\n",
    "* 我们希望路径最短。\n",
    "\n",
    "很容易可以看出来，需要5步。并且所有可能性都是有限的。<br>\n",
    "如果是编程，可以挖掉地雷的位置，然后做最短路径问题。<br>\n",
    "但是我们希望这种游戏是可以重复性游玩的，比如每次游玩的时候，变换地雷和火腿的相对位置，保持狮子的位置不变。目的始终是吃火腿，躲地雷，那么这样才有意思。当然，我不知道能不能做到，因为这种情况，似乎传统编程的难度相当大，而且是编程者来硬解。我们希望是由model自己来学习这个模式。<br>\n",
    "\n",
    "ps:后来发现，Q-Table的本质并不允许更改火腿和地雷的位置。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9bbf90f-a41b-4860-876c-3b04debc1213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L _ _ _ \n",
      "_ # _ _ \n",
      "_ _ $ _ \n",
      "_ _ _ _ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from IPython import display  # 引入 display 模块目的方便程序运行展示\n",
    "\n",
    "\n",
    "def init_env(width = 4,height = 4):\n",
    "    start = (0, 0)\n",
    "    terminal = (int(width/3*2), int(height/2))\n",
    "    hole = (int(width/3),int(height/3))\n",
    "    env = np.array([[\"_ \"] * width] * height)  # 建立一个 4*4 的环境\n",
    "    env[terminal] = \"$ \"  # 目的地\n",
    "    env[hole] = \"# \"  # 陷阱\n",
    "    env[start] = \"L \"  # 小狮子\n",
    "    interaction = \"\"\n",
    "    for i in env:\n",
    "        interaction += \"\".join(i) + \"\\n\"\n",
    "    print(interaction)\n",
    "\n",
    "\n",
    "init_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc39eb81-063f-4b89-92cb-421930a605fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>up</th>\n",
       "      <th>down</th>\n",
       "      <th>left</th>\n",
       "      <th>right</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     up  down  left  right\n",
       "0   0.0   0.0   0.0    0.0\n",
       "1   0.0   0.0   0.0    0.0\n",
       "2   0.0   0.0   0.0    0.0\n",
       "3   0.0   0.0   0.0    0.0\n",
       "4   0.0   0.0   0.0    0.0\n",
       "5   0.0   0.0   0.0    0.0\n",
       "6   0.0   0.0   0.0    0.0\n",
       "7   0.0   0.0   0.0    0.0\n",
       "8   0.0   0.0   0.0    0.0\n",
       "9   0.0   0.0   0.0    0.0\n",
       "10  0.0   0.0   0.0    0.0\n",
       "11  0.0   0.0   0.0    0.0\n",
       "12  0.0   0.0   0.0    0.0\n",
       "13  0.0   0.0   0.0    0.0\n",
       "14  0.0   0.0   0.0    0.0\n",
       "15  0.0   0.0   0.0    0.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def init_q_table(width=4, height=4):\n",
    "    # 定义动作\n",
    "    actions = np.array([\"up\", \"down\", \"left\", \"right\"])\n",
    "    \n",
    "    # 初始化 Q-Table 全为 0\n",
    "    q_table = pd.DataFrame(np.zeros((width * height, len(actions))), columns=actions)\n",
    "    \n",
    "    # 遍历所有的格子，检查边缘情况\n",
    "    for x in range(width):\n",
    "        for y in range(height):\n",
    "            state = x * height + y  # 映射关系\n",
    "            \n",
    "    \n",
    "    return q_table\n",
    "\n",
    "# 调用函数初始化 Q-Table\n",
    "q_table = init_q_table()\n",
    "q_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dc05f6f-2277-45ac-ac44-8f4f10028fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def act_choose(state, q_table, epsilon):\n",
    "    \"\"\"\n",
    "    参数:\n",
    "    state -- 状态\n",
    "    q_table -- Q-Table\n",
    "    epsilon -- 概率值\n",
    "\n",
    "    返回:\n",
    "    action --下一步动作\n",
    "    \"\"\"\n",
    "    state_act = q_table.iloc[state, :]\n",
    "    actions = np.array([\"up\", \"down\", \"left\", \"right\"])\n",
    "\n",
    "    if np.random.uniform() > epsilon or state_act.all() == 0:\n",
    "        action = np.random.choice(actions)\n",
    "    else:\n",
    "        action = state_act.idxmax()\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10a4dcf9-2d9e-46a4-ac4b-0704fa3820d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_feedback(state, action, hole, terminal,width=4,height=4):\n",
    "    \"\"\"\n",
    "    参数:\n",
    "    state -- 状态\n",
    "    action -- 动作\n",
    "    hole -- 陷阱位置\n",
    "    terminal -- 终点位置\n",
    "\n",
    "    返回:\n",
    "    next_state -- 下一状态\n",
    "    reward -- 奖励\n",
    "    end --结束标签\n",
    "    \"\"\"\n",
    "    # 越过边缘惩罚-10.0\n",
    "    # 陷阱惩罚-10.0\n",
    "    # 火腿奖励10.0\n",
    "    \n",
    "    # 行为反馈\n",
    "    reward = 0.0\n",
    "    end = 0\n",
    "    a, b = state\n",
    "    if action == \"up\":\n",
    "        a -= 1\n",
    "        if a < 0:\n",
    "            a = 0\n",
    "            reward = -10.0\n",
    "        next_state = (a, b)\n",
    "        \n",
    "    elif action == \"down\":\n",
    "        a += 1\n",
    "        if a >= height:\n",
    "            a = height-1\n",
    "            reward = -10.0\n",
    "        next_state = (a, b)\n",
    "    elif action == \"left\":\n",
    "        b -= 1\n",
    "        if b < 0:\n",
    "            b = 0\n",
    "            reward = -10.0\n",
    "        \n",
    "        next_state = (a, b)\n",
    "    elif action == \"right\":\n",
    "        b += 1\n",
    "        if b >= width:\n",
    "            b = width-1\n",
    "            reward = -10.0\n",
    "        next_state = (a, b)\n",
    "        \n",
    "\n",
    "    if next_state == terminal:\n",
    "        reward = 10.0\n",
    "        end = 2\n",
    "    elif next_state == hole:\n",
    "        reward = -10.0\n",
    "        end = 1\n",
    "    else:\n",
    "        if reward != -10.0:\n",
    "          reward = -1.0\n",
    "    return next_state, reward, end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0eb8e2a-6da2-47ab-816b-1a37c5bdc9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_q_table(q_table, state, action, next_state, terminal, gamma, alpha, reward,width=4,height=4):\n",
    "    \"\"\"\n",
    "    参数:\n",
    "    q_table -- Q-Table\n",
    "    state -- 状态\n",
    "    action -- 动作\n",
    "    next_state -- 下一状态\n",
    "    terminal -- 终点位置\n",
    "    gamma -- 折损因子\n",
    "    alpha -- 学习率\n",
    "    reward -- 奖励\n",
    "\n",
    "    返回:\n",
    "    q_table -- 更新后的Q-Table\n",
    "    \"\"\"\n",
    "    # Q-Table 的更新函数\n",
    "    x, y = state\n",
    "    next_x, next_y = next_state\n",
    "    q_original = q_table.loc[x * height + y, action]\n",
    "    if next_state != terminal:\n",
    "        q_predict = reward + gamma * q_table.iloc[next_x * height + next_y].max()\n",
    "    else:\n",
    "        q_predict = reward\n",
    "    q_table.loc[x * 4 + y, action] = (1 - alpha) * q_original + alpha * q_predict\n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bf838e6-d4e0-4114-944b-c1787c84eafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_state(end, state, episode, step, q_table,width=4,height=4):\n",
    "    \"\"\"\n",
    "    参数:\n",
    "    end -- 结束标签\n",
    "    state -- 状态\n",
    "    episode -- 迭代次数\n",
    "    step --迭代步数\n",
    "    q_table-- Q-Table\n",
    "    \"\"\"\n",
    "    # 状态可视化辅助函数\n",
    "    terminal = (int(width/3*2), int(height/2))\n",
    "    hole = (int(width/3),int(height/3))\n",
    "    env = np.array([[\"_ \"] * width] * height)\n",
    "    env[terminal] = \"$ \"\n",
    "    env[hole] = \"# \"\n",
    "    env[state] = \"L \"\n",
    "    interaction = \"\"\n",
    "    for i in env:\n",
    "        interaction += \"\".join(i) + \"\\n\"\n",
    "\n",
    "    if state == terminal:\n",
    "        message = \"EPISODE: {}, STEP: {}\".format(episode, step)\n",
    "        interaction += message\n",
    "        display.clear_output(wait=True)  # 清除输出内容\n",
    "        print(interaction)\n",
    "        print(\"\\n\" + \"q_table:\")\n",
    "        print(q_table)\n",
    "        time.sleep(3)  # 在成功到终点时，等待 3 秒\n",
    "    else:\n",
    "        display.clear_output(wait=True)\n",
    "        print(interaction)\n",
    "        print(q_table)\n",
    "        time.sleep(0.3)  # 在这里控制每走一步所需要时间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e78bf7d-1e61-480d-8688-ae171342319d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(max_episodes, alpha, gamma, epsilon,width=4,height=4):\n",
    "    \"\"\"\n",
    "    参数:\n",
    "    max_episodes -- 最大迭代次数\n",
    "    alpha -- 学习率\n",
    "    gamma -- 折损因子\n",
    "    epsilon -- 概率值\n",
    "\n",
    "    返回:\n",
    "    q_table -- 更新后的Q-Table\n",
    "    \"\"\"\n",
    "    q_table = init_q_table()\n",
    "    terminal = (int(width/3*2), int(height/2))\n",
    "    hole = (int(width/3),int(height/3))\n",
    "    episodes = 0\n",
    "    while episodes <= max_episodes:\n",
    "        step = 0\n",
    "        state = (0, 0)\n",
    "        end = 0\n",
    "        show_state(end, state, episodes, step, q_table)\n",
    "        while end == 0:\n",
    "            x, y = state\n",
    "            act = act_choose(x * height + y, q_table, epsilon)  # 动作选择\n",
    "            next_state, reward, end = env_feedback(state, act, hole, terminal)  # 环境反馈\n",
    "            q_table = update_q_table(\n",
    "                q_table, state, act, next_state, terminal, gamma, alpha, reward\n",
    "            )  # q-table 更新\n",
    "            state = next_state\n",
    "            step += 1\n",
    "            show_state(end, state, episodes, step, q_table)\n",
    "        if end == 2:\n",
    "            episodes += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d62fdba-932a-43a4-8b9c-afe146b52905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_ _ _ _ \n",
      "_ # _ _ \n",
      "_ _ L _ \n",
      "_ _ _ _ \n",
      "EPISODE: 15, STEP: 6\n",
      "\n",
      "q_table:\n",
      "           up      down      left     right\n",
      "0  -11.112960  4.570999 -7.075289  0.966886\n",
      "1   -8.000000 -9.984000 -1.894645  5.274991\n",
      "2   -9.920000  7.913718 -1.853696 -0.960000\n",
      "3   -4.454900  1.122685  4.923750  0.000000\n",
      "4   -1.651200  6.198483 -9.999360 -9.600000\n",
      "5    0.000000  0.000000  0.000000  0.000000\n",
      "6    4.856744  9.984000 -8.000000 -0.460937\n",
      "7    2.892618  4.800000  6.112000  0.000000\n",
      "8   -0.992000 -0.998400 -4.346880  7.999866\n",
      "9   -8.000000  2.468352  5.835090  9.999974\n",
      "10   0.000000  0.000000  0.000000  0.000000\n",
      "11   2.656000  4.007680  9.600000 -2.240000\n",
      "12   4.798258  0.000000 -8.000000 -0.960000\n",
      "13  -0.800000  0.000000  0.000000  5.912320\n",
      "14   9.600000 -8.000000 -0.960000  3.468288\n",
      "15   7.342080 -8.000000 -0.800000 -9.600000\n"
     ]
    }
   ],
   "source": [
    "q_learning(max_episodes=15, alpha=0.8, gamma=0.9, epsilon=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d98049-98f4-4326-b58b-6af92efb0302",
   "metadata": {},
   "source": [
    "到了后面，我的狮子路线是固定的。<br>\n",
    "\n",
    "它把所有位置的所有可能都列出来，存储量还是很大的，如果下五子棋也这么搞，也就是要记录所有的残局情况。<br>\n",
    "\n",
    "然后一次次解出残局的最优解。<br>\n",
    "\n",
    "用数学函数来说，实际上就是不断修正函数，来让函数来拟合某一个最优解。<br>\n",
    "\n",
    "有缺点，这里，每次更换目标都需要重新训练，而且每一次都不能找出所有最优解，只是找出一个局部最优解。【实际上这点也有点为难人了。】<br>\n",
    "\n",
    "但是，实际上，这个应该在五子棋上面表现不错。只是参数量估计相当大。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b50aba8-4402-42af-85ef-52cf5ee1f505",
   "metadata": {},
   "source": [
    "## 让游戏更有意思一些。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9d43ca-f3ef-410d-b695-8fd0c190b717",
   "metadata": {},
   "source": [
    "我把狮子的出生点随机化了(陷阱和hole除外）,然后把地图变成6x6<br>\n",
    "\n",
    "实际上没有改动算法根基，如果要动火腿和陷阱，目前的Q-Table就不能是静态的。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5146e8d-b19f-4759-a26d-2276fcdaee33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def random_start_q_learning(max_episodes, alpha, gamma, epsilon,width=4,height=4):\n",
    "    \"\"\"\n",
    "    参数:\n",
    "    max_episodes -- 最大迭代次数\n",
    "    alpha -- 学习率\n",
    "    gamma -- 折损因子\n",
    "    epsilon -- 探索概率值\n",
    "\n",
    "    返回:\n",
    "    q_table -- 更新后的Q-Table\n",
    "    \"\"\"\n",
    "    q_table = init_q_table(width=width,height=height)\n",
    "    terminal = (int(width/3*2), int(height/2))\n",
    "    hole = (int(width/3),int(height/3))\n",
    "    \n",
    "    # 定义网格的所有合法起点，排除 hole 和 terminal\n",
    "    valid_start_positions = [(i, j) for i in range(width) for j in range(height) if (i, j) != hole and (i, j) != terminal]\n",
    "\n",
    "    episodes = 0\n",
    "    while episodes <= max_episodes:\n",
    "        step = 0\n",
    "\n",
    "        # 随机选择起始点，排除 hole 和 terminal\n",
    "        state = random.choice(valid_start_positions)\n",
    "\n",
    "        end = 0\n",
    "        show_state(end, state, episodes, step, q_table,width=height,height=height)\n",
    "        while end == 0:\n",
    "            x, y = state\n",
    "            act = act_choose(x * width + y, q_table, epsilon)  # 动作选择\n",
    "            next_state, reward, end = env_feedback(state, act, hole, terminal,width=width,height=height)  # 环境反馈\n",
    "            q_table = update_q_table(\n",
    "                q_table, state, act, next_state, terminal, gamma, alpha, reward,width=width,height=height\n",
    "            )  # q-table 更新\n",
    "            state = next_state\n",
    "            step += 1\n",
    "            show_state(end, state, episodes, step, q_table,width=width,height=height)\n",
    "        if end == 2:\n",
    "            episodes += 1\n",
    "\n",
    "    return q_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f775450-c2e7-48ee-b516-597cb5b8ab26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_ _ _ _ _ L \n",
      "_ _ _ _ _ _ \n",
      "_ _ # _ _ _ \n",
      "_ _ _ _ _ _ \n",
      "_ _ _ $ _ _ \n",
      "_ _ _ _ _ _ \n",
      "\n",
      "          up      down      left     right\n",
      "0  -6.588574  1.189814 -6.588574  2.050207\n",
      "1  -6.938713  3.389120  0.516046  0.750078\n",
      "2  -9.920000 -0.960000  2.050208 -0.960000\n",
      "3  -8.192375  0.151714  0.845182 -0.519958\n",
      "4   0.851645 -3.031040 -5.253015  3.041588\n",
      "5  -8.561375  1.431761  1.291495 -8.698250\n",
      "6   0.880065 -8.275200  2.985853  1.651458\n",
      "7  -0.034564  4.876800 -0.291594 -2.086400\n",
      "8   1.019574  2.656000 -0.235093  1.145409\n",
      "9   0.771260 -0.069478 -2.284800 -5.815538\n",
      "10  0.000000  0.000000  0.000000  0.000000\n",
      "11  2.701957  0.192000 -7.424000  1.195264\n",
      "12 -1.651200 -1.280000 -8.000000  0.852122\n",
      "13  1.337665  4.800000 -0.800000  4.960000\n",
      "14 -7.584000 -2.400000  0.800000  0.000000\n",
      "15  5.836800  7.840000  0.000000  0.000000\n",
      "16  1.195264 -2.400000  2.771200  4.800000\n",
      "17  0.961280 -0.800000 -0.800000 -2.240000\n",
      "18 -1.376000  0.000000  0.000000  0.000000\n",
      "19  0.000000  0.000000  0.000000  0.000000\n",
      "20 -0.800000 -8.000000  8.000000 -0.800000\n",
      "21 -1.376000 -0.800000 -0.800000 -8.000000\n",
      "22  0.000000  0.000000  0.000000 -0.800000\n",
      "23  8.000000 -8.000000 -0.800000 -0.800000\n",
      "24 -0.800000 -8.000000 -0.800000 -0.800000\n",
      "25 -0.800000 -8.000000 -0.800000 -8.000000\n",
      "26  0.000000  0.000000  0.000000  0.000000\n",
      "27  0.000000  0.000000  0.000000  0.000000\n",
      "28  0.000000  0.000000  0.000000  0.000000\n",
      "29  0.000000  0.000000  0.000000  0.000000\n",
      "30  0.000000  0.000000  0.000000  0.000000\n",
      "31  0.000000  0.000000  0.000000  0.000000\n",
      "32  0.000000  0.000000  0.000000  0.000000\n",
      "33  0.000000  0.000000  0.000000  0.000000\n",
      "34  0.000000  0.000000  0.000000  0.000000\n",
      "35  0.000000  0.000000  0.000000  0.000000\n"
     ]
    }
   ],
   "source": [
    "random_start_q_learning(max_episodes=100, alpha=0.8, gamma=0.9, epsilon=0.9,width=6,height=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa094fe2-0887-4ec3-a3e7-afdffc53b471",
   "metadata": {},
   "source": [
    "缺少徘徊惩罚，如果陷入徘徊会导致训练卡死。<br>\n",
    "至少效率是很低的<br>\n",
    "\n",
    "但是目前的架构似乎是加不进来徘徊惩罚的，因为徘徊是二步，四步等等的循环。<br>\n",
    "\n",
    "它不是单独一步的惩罚，任何单独一步可能都是正常的，但是连起来就变成了循环，就不正常。<br>\n",
    "\n",
    "待解决。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7e4d5b-32c6-4a2d-aa2b-8847888365c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
